{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import math \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def replicate(layer,N):\n",
    "    return nn.ModuleList([layer for _ in range(N)])\n",
    "\n",
    "class Encode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Encode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,mask)\n",
    "        return X\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self,features,eps=1e-16):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2=nn.Parameter(torch.ones(features))\n",
    "        self.b_2=nn.Parameter(torch.zeros(features))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout_rate, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        \n",
    "        # Layer normalization component\n",
    "        self.norm = LayerNorm(size, eps=eps)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, sublayer_output):\n",
    "        \"Apply residual connection followed by layer normalization\"\n",
    "        # Residual connection\n",
    "        added_output = x + self.dropout(sublayer_output)\n",
    "        \n",
    "        # Layer normalization\n",
    "        return self.norm(added_output)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        d_ff: the number of features of the feedforward network model.\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # Two linear layers with a ReLU activation in between\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key,value,mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Linear layers\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.d_k**0.5\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_probs, V)\n",
    "\n",
    "        # Concatenate heads and pass through final linear layer\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_k * self.num_heads)\n",
    "        output = self.fc_out(attention_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Multi_Head_Attention,AddNorm):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attn = Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.AddNorm=AddNorm\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,mask))\n",
    "        x=self.AddNorm(x, self.feed_forward(x))\n",
    "        return x  \n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model,vocab):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)   \n",
    "    \n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.dmodel=d_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        n=x.shape[1]\n",
    "        div_term = torch.exp(torch.arange(0., self.dmodel, 2) * -(math.log(10000.0) / self.dmodel))\n",
    "\n",
    "        positions = torch.arange(n).unsqueeze(1).float()\n",
    "        div_term = div_term.unsqueeze(0)\n",
    "        sin_vals = torch.sin(positions * div_term)\n",
    "        cos_vals = torch.cos(positions * div_term)\n",
    "\n",
    "        ZZ = torch.empty(n, self.dmodel)\n",
    "        ZZ[:, 0::2] = sin_vals\n",
    "        ZZ[:, 1::2] = cos_vals\n",
    "        return x+ZZ.requires_grad_(False)\n",
    "    \n",
    "def make_model(src_vocab,tgt_vocab,N=6,d_model=512,h=8,dropout=0.1,d_ff=2048):\n",
    "    \n",
    "    model=Encoder_Decoder(\n",
    "        Encode(Encoder(d_model,FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        Decode(Decoder(d_model, FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),MultiHeadAttention(d_model,h)),N),\n",
    "        nn.Sequential(Embedding(d_model,src_vocab),Positional_Encoding(d_model)),\n",
    "        nn.Sequential(Embedding(d_model,tgt_vocab),Positional_Encoding(d_model)),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,d_model,tgt_vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear=nn.Linear(d_model,tgt_vocab)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.linear(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "\n",
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self, Encode,Decode,src_embed, tgt_embed, generator):\n",
    "        super(Encoder_Decoder, self).__init__()\n",
    "        self.Encode=Encode\n",
    "        self.Decode=Decode\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.generator=generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.Encode(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.Decode(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "class Decode(nn.Module):\n",
    "\n",
    "    def __init__(self,layer,N):\n",
    "        super(Decode, self).__init__()\n",
    "        self.layer=replicate(layer,N)\n",
    "        self.N=N\n",
    "\n",
    "    def forward(self,X,y,src_mask,tgt_mask):\n",
    "        for layer in self.layer:\n",
    "            X=layer(X,y,src_mask,tgt_mask)\n",
    "        return X\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, size, FeedForward, Self_Multi_Head_Attention,Encoder_Multi_Head_Attention,AddNorm):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.self_attn = Self_Multi_Head_Attention\n",
    "        self.feed_forward = FeedForward\n",
    "        self.encoder_attention=Encoder_Multi_Head_Attention\n",
    "        self.AddNorm=AddNorm\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self,x,m,src_mask,tgt_mask):\n",
    "        x=self.AddNorm(x, self.self_attn(x,x,x,tgt_mask))\n",
    "        x=self.AddNorm(x, self.encoder_attention(x,m,m,src_mask))\n",
    "        x=self.AddNorm(x, self.feed_forward(x))\n",
    "        return x   \n",
    "    \n",
    "def make_model(src_vocab,tgt_vocab,N=6,d_model=512,h=8,dropout=0.1,d_ff=2048):\n",
    "    \n",
    "    model=Encoder_Decoder(\n",
    "        Encode(Encoder(d_model,FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        Decode(Decoder(d_model, FeedForward(d_model, d_ff, dropout=dropout), MultiHeadAttention(d_model, h),MultiHeadAttention(d_model,h),AddNorm(d_model,dropout, eps=1e-6)),N),\n",
    "        nn.Sequential(Embedding(d_model,src_vocab),Positional_Encoding(d_model)),\n",
    "        nn.Sequential(Embedding(d_model,tgt_vocab),Positional_Encoding(d_model)),\n",
    "        Generator(d_model,tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Encode.layer.0.self_attn.query.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.query.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.key.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.key.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.value.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.value.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.fc_out.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.self_attn.fc_out.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.feed_forward.w_1.weight, Size: torch.Size([2048, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.feed_forward.w_1.bias, Size: torch.Size([2048]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.feed_forward.w_2.weight, Size: torch.Size([512, 2048]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.feed_forward.w_2.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.AddNorm.norm.a_2, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Encode.layer.0.AddNorm.norm.b_2, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.query.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.query.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.key.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.key.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.value.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.value.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.fc_out.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.self_attn.fc_out.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.feed_forward.w_1.weight, Size: torch.Size([2048, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.feed_forward.w_1.bias, Size: torch.Size([2048]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.feed_forward.w_2.weight, Size: torch.Size([512, 2048]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.feed_forward.w_2.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.query.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.query.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.key.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.key.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.value.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.value.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.fc_out.weight, Size: torch.Size([512, 512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.encoder_attention.fc_out.bias, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.AddNorm.norm.a_2, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: Decode.layer.0.AddNorm.norm.b_2, Size: torch.Size([512]) ,f\"Requires Grad: True\"\n",
      "Name: src_embed.0.embedding.weight, Size: torch.Size([11, 512]) ,f\"Requires Grad: True\"\n",
      "Name: tgt_embed.0.embedding.weight, Size: torch.Size([11, 512]) ,f\"Requires Grad: True\"\n",
      "Name: generator.linear.weight, Size: torch.Size([11, 512]) ,f\"Requires Grad: True\"\n",
      "Name: generator.linear.bias, Size: torch.Size([11]) ,f\"Requires Grad: True\"\n"
     ]
    }
   ],
   "source": [
    "test_model = make_model(11, 11, 2)\n",
    "for name, param in test_model.named_parameters():\n",
    "    print(f'Name: {name}, Size: {param.size()} ,f\"Requires Grad: {param.requires_grad}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self,src,tgt,pad=2):\n",
    "        self.src=src\n",
    "        self.src_mask=(src!=pad).unsqueeze(-2)\n",
    "        self.tgt=tgt[:,:-1]\n",
    "        self.tgt_y=tgt[:,1:]\n",
    "        self.tgt_mask=self.make_std_mask(self.tgt,pad)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt,pad):\n",
    "        tgt_mask=(tgt!=pad).unsqueeze(-2)\n",
    "        tgt_mask=tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V,batch_size,n_batches):\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # Get the data and labels for the current batch\n",
    "        data=torch.randint(1,V,size=(batch_size,10))\n",
    "        data[:,0]=1\n",
    "        src=data.requires_grad_(False).clone().detach()\n",
    "        tgt=data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src,tgt,pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# numpy_array = out.detach().numpy()\n",
    "\n",
    "# # Save the numpy array to a CSV file\n",
    "# np.savetxt('tensor.csv', numpy_array[0], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step_num,d_model,factor,warmup_steps):\n",
    "    if step_num==0:\n",
    "        step_num=1\n",
    "    return d_model**(-0.5) * min(step_num**(-0.5), step_num * warmup_steps**(-1.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(443.7314, grad_fn=<SumBackward0>)\n",
      "tensor(428.4224, grad_fn=<SumBackward0>)\n",
      "tensor(441.3571, grad_fn=<SumBackward0>)\n",
      "tensor(437.3248, grad_fn=<SumBackward0>)\n",
      "tensor(421.9269, grad_fn=<SumBackward0>)\n",
      "tensor(430.6405, grad_fn=<SumBackward0>)\n",
      "tensor(430.4940, grad_fn=<SumBackward0>)\n",
      "tensor(422.9593, grad_fn=<SumBackward0>)\n",
      "tensor(394.9222, grad_fn=<SumBackward0>)\n",
      "tensor(414.5292, grad_fn=<SumBackward0>)\n",
      "tensor(377.6584, grad_fn=<SumBackward0>)\n",
      "tensor(327.2587, grad_fn=<SumBackward0>)\n",
      "tensor(255.7952, grad_fn=<SumBackward0>)\n",
      "tensor(36.1524, grad_fn=<SumBackward0>)\n",
      "tensor(3.0189, grad_fn=<SumBackward0>)\n",
      "tensor(0.5898, grad_fn=<SumBackward0>)\n",
      "tensor(2.2115, grad_fn=<SumBackward0>)\n",
      "tensor(0.9498, grad_fn=<SumBackward0>)\n",
      "tensor(0.3483, grad_fn=<SumBackward0>)\n",
      "tensor(0.1084, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "test_model = make_model(11, 11, 2)\n",
    "criterion=LabelSmoothing(11,0,0.0)\n",
    "optimizer = torch.optim.Adam(test_model.parameters(), lr=1)\n",
    "lr_lambda = lambda step: rate(step,512,1,4000)\n",
    "\n",
    "# Create a LambdaLR scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batch in data_gen(11,20,30):  # Assume data_loader is an iterator that provides batches of data\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get the data and labels\n",
    "        \n",
    "        # Forward pass\n",
    "        out = test_model.forward(batch.src,batch.tgt,batch.src_mask,batch.tgt_mask)\n",
    "        out=test_model.generator(out)\n",
    "        # Compute loss\n",
    "        loss = criterion(out.contiguous().view(-1,out.size(-1)),batch.tgt_y.contiguous().view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 9, 3, 4, 9, 4, 7, 8, 9]])\n",
      "tensor([[0, 1, 9, 3, 4, 9, 4, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "#test_model = make_model(11, 11, 2)\n",
    "src=torch.LongTensor([[0,1,9,3,4,9,4,7,8,9]])\n",
    "max_len=src.shape[1]\n",
    "src_mask=torch.ones(1,1,max_len)\n",
    "memory = test_model.encode(src, src_mask)\n",
    "ys = torch.zeros(1, 1).fill_(0).type_as(src.data)\n",
    "for i in range(10 - 1):\n",
    "    out = test_model.decode(\n",
    "        memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "    )\n",
    "    prob = test_model.generator(out[:, -1])\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.data[0]\n",
    "    ys = torch.cat(\n",
    "        [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "    )\n",
    "print(src)\n",
    "print(ys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
